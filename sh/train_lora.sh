#torchrun train_lora.py --output_dir D:/jupyter_notebook/output/classification/nli/lora/poliglot_1.3b --train_data D:/jupyter_notebook/data/NLI/dev.jsonl --val_data D:/jupyter_notebook/data/NLI/dev.jsonl --logging_term 100 --epochs 5 --eval_epoch 1 --batch_size 4 --warmup 1 --ptm_path EleutherAI/polyglot-ko-1.3b --early_stop True --early_stop_metric loss --early_stop_metric_is_max_better False --save_model_every_epoch True --patience 1 --lr 2e-5 --fp16 True --fp16_model False --accumulation_steps 1 --n_labels 3 

torchrun train_lora.py --output_dir D:/jupyter_notebook/output/classification/nli/lora/t5-small --train_data D:/jupyter_notebook/data/NLI/dev.jsonl --val_data D:/jupyter_notebook/data/NLI/dev.jsonl --logging_term 100 --epochs 5 --eval_epoch 1 --batch_size 4 --warmup 1 --ptm_path ../../plm/t5-small --early_stop True --early_stop_metric loss --early_stop_metric_is_max_better False --save_model_every_epoch True --patience 1 --lr 2e-5 --fp16 False --fp16_model False --accumulation_steps 1 --n_labels 3 

